import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import numpy as np
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import json
from data import *

# GPU æ£€æµ‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"à¸…^â€¢ï»Œâ€¢^à¸…  å–µå–µ~ è®¾å¤‡æ£€æµ‹ä¸­... ä½¿ç”¨çš„æ˜¯ {'GPUâœ¨' if torch.cuda.is_available() else 'CPUğŸ”§'} å“¦ï¼")

# å®šä¹‰ç¥ç»ç½‘ç»œ
class RegressionModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(RegressionModel, self).__init__()
        print(f"âœ¨ ä¹–ä¹–~ æ­£åœ¨åˆ›å»º {num_layers} å±‚çš„å–µå–µç¥ç»ç½‘ç»œï¼Œéšè—å±‚å¤§å°æ˜¯ {hidden_size} å“¦ï¼")
        layers = []
        layers.append(nn.Linear(input_size, hidden_size))
        layers.append(nn.ReLU())
        for _ in range(num_layers - 1):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(hidden_size, 1))  # è¾“å‡ºå±‚
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)
    
    def save(self):
        torch.save(self.state_dict(), self.file)
        print("ğŸ’¾ å–µå‘œï¼æ¨¡å‹å·²ç»å­˜å¥½å•¦ï¼")

    def load(self):
        if os.path.exists(self.file):
            self.load_state_dict(torch.load(self.file))
            print("âœ… å·²åŠ è½½æ¨¡å‹æƒé‡ï¼Œå–µ~å¿«æ¥çœ‹çœ‹å®ƒå˜å¼ºäº†æ²¡ï¼")

# æ—©åœæœºåˆ¶
class EarlyStopping:
    def __init__(self, patience=10):
        self.patience = patience
        self.best_loss = float("inf")
        self.counter = 0
        print(f"ğŸš€ æ—©åœæœºåˆ¶å¯åŠ¨ï¼è€å¿ƒå€¼è®¾ä¸º {patience}ï¼Œè¦æ˜¯å­¦ä¹ å¤ªä¹…æ²¡æœ‰è¿›æ­¥ï¼Œå–µå°±ä¼šå–Šåœå“¦ï¼")

    def step(self, loss):
        if loss < self.best_loss:
            self.best_loss = loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            print(f"ğŸ¥º å–µå‘œâ€¦â€¦ éªŒè¯æŸå¤± {loss:.6f} æ²¡æœ‰å˜å¥½å‘¢ï¼Œå·²ç» {self.counter}/{self.patience} è½®äº†å–µï¼")
            return self.counter >= self.patience

# è¶…å‚æ•°ä¼˜åŒ–
def objective(trial):
    hidden_size = trial.suggest_int("hidden_size", 16, 128)
    num_layers = trial.suggest_int("num_layers", 1, 3)
    learning_rate = trial.suggest_loguniform("lr", 1e-4, 1e-2)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])

    print(f"âœ¨ å¼€å§‹å–µå–µè¯•éªŒ {trial.number}ï¼šhidden_size={hidden_size}, num_layers={num_layers}, lr={learning_rate:.6f}, batch_size={batch_size}")

    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    model = RegressionModel(input_size=X.shape[1], hidden_size=hidden_size, num_layers=num_layers).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    early_stopping = EarlyStopping(patience=10)

    # è®­ç»ƒæ¨¡å‹
    for epoch in range(100):
        model.train()
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                val_loss += criterion(outputs, batch_y).item()
        val_loss /= len(val_loader)

        print(f"ğŸ“Š è¯•éªŒ {trial.number} - Epoch {epoch+1}: éªŒè¯æŸå¤± {val_loss:.6f}")

        if early_stopping.step(val_loss):
            print("ğŸ’¤ æ—©åœè§¦å‘ï¼å–µä¸ç»ƒäº†ï¼")
            break

    return val_loss


BEST_PARAMS_FILE = "boston/best_hyperparams.json"
def save_best_hyperparams(params):
    """ä¿å­˜æœ€ä¼˜è¶…å‚æ•°åˆ° JSON æ–‡ä»¶"""
    with open(BEST_PARAMS_FILE, "w") as f:
        json.dump(params, f, indent=4)
    print("ğŸ“âœ¨ æœ€ä¼˜è¶…å‚æ•°å·²ç»å­˜å‚¨åˆ° 'best_hyperparams.json' å•¦ï¼ä¸‹æ¬¡å¯ä»¥ç›´æ¥åŠ è½½å“¦ï¼(à¸…^Ï‰^à¸…)")

def load_best_hyperparams():
    """ä» JSON æ–‡ä»¶åŠ è½½æœ€ä¼˜è¶…å‚æ•°"""
    if os.path.exists(BEST_PARAMS_FILE):
        with open(BEST_PARAMS_FILE, "r") as f:
            params = json.load(f)
        print("ğŸ‰ å·²æˆåŠŸåŠ è½½ä¹‹å‰æ‰¾åˆ°çš„æœ€ä¼˜è¶…å‚æ•°ï¼ä¸ç”¨å†ä»é›¶å¼€å§‹å•¦ï¼(â‰§â–½â‰¦)ï¾‰")
        return params
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å­˜å‚¨çš„è¶…å‚æ•°å“¦ï¼Œå…ˆæ¥æ¢ç´¢æœ€ä¼˜å‚æ•°å§ï¼(ï¼›Â´Ğ”`)")
        return None


