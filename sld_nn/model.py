import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
import optuna
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from data import *

# è®¾ç½®è®¾å¤‡ï¼ˆä½¿ç”¨ GPU åŠ é€Ÿï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ====================
# 1. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
# ====================
class RegressionNet(nn.Module):
    def __init__(self, input_size=385, hidden_size=128):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = 1
        self.file = "sld_nn/model.pth"

        self.fc1 = nn.Linear(self.input_size, self.hidden_size)
        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)
        self.fc3 = nn.Linear(self.hidden_size, self.output_size)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)  # æ·»åŠ  Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)  # æ·»åŠ  Dropout
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def save(self):
        torch.save(self.state_dict(), self.file)
        
    def load(self):
        if os.path.exists(self.file):
            self.load_state_dict(torch.load(self.file))
            print("âœ… å·²åŠ è½½æ¨¡å‹æƒé‡")


# å½’ä¸€åŒ–æ•°æ®
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# è½¬æ¢ä¸º Tensor
X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# åˆ›å»º DataLoader
batch_size = 64
train_dataset = Data.TensorDataset(X_train, y_train)
test_dataset = Data.TensorDataset(X_test, y_test)
train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# ====================
# 3. è®­ç»ƒ & è¯„ä¼°å‡½æ•°
# ====================
def train_model(model, optimizer, criterion, train_loader, test_loader, epochs=50, patience=5):
    model.to(device)
    best_loss = float("inf")
    no_improve_epochs = 0

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)

            optimizer.zero_grad()
            predictions = model(batch_x)
            loss = criterion(predictions, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # æ¢¯åº¦è£å‰ª
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # éªŒè¯é›†è¯„ä¼°
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                predictions = model(batch_x)
                loss = criterion(predictions, batch_y)
                val_loss += loss.item()
        
        val_loss /= len(test_loader)

        print(f"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # æ—©åœæœºåˆ¶
        if val_loss < best_loss:
            best_loss = val_loss
            no_improve_epochs = 0
            model.save()
        else:
            no_improve_epochs += 1
            if no_improve_epochs >= patience:
                print("â¹ï¸  æ—©åœè§¦å‘ï¼Œè®­ç»ƒåœæ­¢ï¼")
                break

    return best_loss

# ====================
# 4. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰
# ====================
def objective(trial):
    hidden_size = trial.suggest_int("hidden_size", 64, 512)
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-5, 1e-2)

    # åˆ›å»ºæ¨¡å‹
    model = RegressionNet(hidden_size=hidden_size).to(device)
    
    # é€‰æ‹©ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # è®­ç»ƒæ¨¡å‹
    val_loss = train_model(model, optimizer, criterion, train_loader, test_loader, epochs=50)
    
    return val_loss

# è¿è¡Œ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=20)

# è¾“å‡ºæœ€ä½³å‚æ•°
best_params = study.best_params
print(f"âœ… æœ€ä½³è¶…å‚æ•°: {best_params}")

# ====================
# 5. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# ====================
print("ğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
best_model = RegressionNet(hidden_size=best_params["hidden_size"]).to(device)
best_optimizer = optim.AdamW(best_model.parameters(), lr=best_params["learning_rate"], weight_decay=1e-2)
best_criterion = nn.MSELoss()

train_model(best_model, best_optimizer, best_criterion, train_loader, test_loader, epochs=100)

print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜ã€‚")